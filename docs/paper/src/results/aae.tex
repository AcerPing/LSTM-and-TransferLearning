\section{AAEを用いた構造提案} \label{sec:result_aae}

本節ではAAEを適用した際の結果を示す。

AAEを適用して構造生成を行い、生成された構造が活性を持つ確率の分布や、生成された構造の骨格について検討した。

\subsection{使用したアーキテクチャ} \label{sec:aae_architecture}

\tbref{tb:aae_parameter_search}にAAEのアーキテクチャで検討したパラメータを示す。
パラメータのうち、層の種類・層の数・隠れ層の数は全ての候補の組み合わせを試すグリッドサーチを行った。
Conv1Dのカーネルサイズについては先行研究と同様の値を利用した。
\begin{table}[tbp]
    \centering
\caption{AAEで検討したパラメータ} \label{tb:aae_parameter_search}
\begin{tabular}{lll}\hline
& 項目 & パラメータ候補 \\\hline
層の種類 & Encoderの入力層 & Embedding, One-Hot Encoding \\
& Encoderの隠れ層 & Conv1D \\
& Encoderの出力層 & Tanh \\
& Decoderの入力層 & Dense \\
& Decoderの隠れ層 & LSTM, GRU \\
& Decoderの出力層 & Softmax \\
& Discriminatorの入力層 & Dense \\
& Discriminatorの隠れ層 & Dense \\
& Discriminatorの出力層 & Softmax \\\hline
各層のパラメータ & Embeddingの次元数 & 16 \\
& Denseの次元数 & 32, 64, 128 \\
& LSTMの次元数 & 16, 32, 64 \\
& GRUの次元数 & 16, 32, 64 \\
& Conv1Dのチャネル数 & 16, 32, 64 \\
& Conv1Dのフィルタサイズ & 9, 10 \\
& 活性化関数 & ReLU, SELU \\\hline
最適化 & バッチサイズ & 256, 512 \\
& 最適化手法 & SGD, Adam \\
& 学習率 & 1e-3, 5e-4, 2.5e-4, 1e-4, 5e-5 \\\hline
\end{tabular}
\end{table}

\tbref{tb:aae_model_detail}に採用されたパラメータを示した。
最適化手法にはEncoder、DecoderにはAdamを学習率\SI{5e-4}で用いた。
Discriminatorには通常のSGDを用い、学習率は\SI{2.5e-4}を採用した。
Encoder、DecoderとDiscriminatorの間で最適化手法を変えている理由には、
EncoderとDecoderの学習が難しく、Discriminatorの学習が容易だという背景がある。
\begin{table}[tbp]
    \centering
    \caption{モデルのアーキテクチャ} \label{tb:aae_model_detail}
    \begin{tabular}{llclc}\hline
     & 層の種類 & 層数 & パラメータ & 活性化関数 \\\hline
     Encoder & Embedding & 1 & 次元数16 & --- \\
     & Conv1D & 3 & チャネル数64, フィルタサイズ$[9, 9, 10]$ & SELU \\
     & Dense & 2 & 次元数32 & ReLU \\
     & Dense & 1 & --- & Softmax \\\hline
     Decoder & Dense & 1 & 次元数32 & ReLU \\
     & GRU & 2 & 次元数64 & ReLU \\
     & Dense & 2 & 次元数32 & ReLU \\
     & Dense & 1 & --- & Softmax \\\hline
    Discriminator & Dense & 3 & 次元数32 & ReLU \\
    & Dense & 1 & --- & Softmax \\\hline
    \end{tabular}
\end{table}

GANの枠組みはひとつの目的関数をEncoder・Decoder・Discriminatorの3つが使用する。
Discriminatorの学習が著しく進んでしまうと、EncoderとDecoderの学習が進まなくなる。
そこで、EncoderとDecoderには収束が早いAdamを、Discriminatorには単純なSGDを利用した。

\subsection{生成構造の評価方法} \label{sec:structure_check}

生成された構造の評価には、生成された構造が活性を持つ確率および生成された構造が持つ骨格を考慮した。

生成された構造が活性を持つ確率は、Random Forestで構築した判別モデルの予測確率を利用した。
Random Forestの回帰木の数は$900$とし、記述子にはMorgan Fingerprintを利用した。
Morgan Fingerprintの半径は$2$、ビット数は$512$とした。

Random Forestで構築した判別モデルの精度を5-foldクロスバリデーションで評価したところ、$\mathrm{AUC} = 0.692$であった。
この値自体は高くはないため、予測確率の値そのものを信用するのは難しい。
しかし、予測確率の分布はある程度信頼できると考え、これを評価値のひとつとした。

生成構造の構造式としての評価は、検証用データとして訓練には用いていない既知医薬品が持つ分子骨格と比較することで行った。

\subsection{結果と考察}

\subsubsection{潜在変数の可視化}

\figref{fig:2d_latent}に、ChEMBL22、ADRA2AそれぞれをAAEの潜在変数空間中にプロットした結果を示す。
ChEMBL22は潜在変数空間中で2次元のガウス分布全体に射影されたのに対し、
ADRA2Aは偏った場所に射影された。
これは、AAEによって活性のある分子同士が近づくように学習が行われたことを示す。
ADRA2A付近からサンプリングを行い、Decoderに通せば活性のある新規構造を生成することができた。
\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.75\columnwidth]{../resource/aae/2d_latent.png}
    \caption{潜在変数へのChEMBL22とADRA2Aの射影} \label{fig:2d_latent}
\end{figure}

\subsubsection{生成構造の活性予測値分布}

\figref{fig:aae_prob_hist}にChEMBL22、ADRA2Aそれぞれの分布からサンプリングして得た構造それぞれに対し、
Random Forestを用いて予測した活性予測値のヒストグラムを示す。横軸が活性のある確率を、縦軸が正規化された度数を表す。
ADRA2A付近からサンプリングした構造は、ChEMBL22付近からサンプリングした構造に比べ活性予測値がより高い傾向にあった。
\begin{figure}[tbp]
    \centering
    \includegraphics[width=0.75\columnwidth]{../resource/aae/prob_hist.png}
    \caption{AAEで生成された構造の活性予測値分布} \label{fig:aae_prob_hist}
\end{figure}

\subsubsection{生成構造の例}

図\ref{fig:aae_identical}, \ref{fig:aae_similar}に提案手法によって生成された構造の一例を示す。
これらはRandom Forestによって活性のある確率が$0.8$以上とされた構造である。
\figref{fig:aae_identical}は検証構造と全く同一の構造を生成できたパターン、
\figref{fig:aae_similar}は検証構造と同様の骨格を持った分子を生成できたパターンである。
生成された構造それぞれについて、半径$2$のMorgan Fingerprintで類似度を計算し、
最も類似している検証構造と併せて示した。
提案手法によって新規医薬品構造候補を生成できたことが確認された。
\begin{figure}[tbp]
    \begin{minipage}[h]{0.49\hsize}
    \centering
    \includegraphics[width=0.9\columnwidth]{../resource/aae/match1.png} 
    \end{minipage}
    \begin{minipage}[h]{0.49\hsize}    
    \centering
    \includegraphics[width=0.6\columnwidth]{../resource/aae/match2.png}
    \end{minipage}
\caption{検証構造と同一の構造を生成できたパターン} \label{fig:aae_identical}
\end{figure}
\begin{figure}[tbp]
    \begin{minipage}[b]{0.49\hsize}
    \centering
    \includegraphics[width=0.9\columnwidth]{../resource/aae/sim1_valid.png} 
    \end{minipage}
    \begin{minipage}[b]{0.49\hsize}    
    \centering
    \includegraphics[width=0.9\columnwidth]{../resource/aae/sim1_new.png}
    \end{minipage}

    \begin{minipage}[b]{0.49\hsize}    
        \centering
        \includegraphics[width=0.8\columnwidth]{../resource/aae/sim2_valid.png}
        \end{minipage}
    \begin{minipage}[b]{0.49\hsize}    
        \centering
        \includegraphics[width=0.9\columnwidth]{../resource/aae/sim2_new.png}
    \end{minipage}

    \begin{minipage}[b]{0.49\hsize}    
        \centering
        \includegraphics[width=0.8\columnwidth]{../resource/aae/sim3_valid.png}
        \subcaption{検証構造}
    \end{minipage}
    \begin{minipage}[b]{0.49\hsize}    
        \centering
        \includegraphics[width=0.8\columnwidth]{../resource/aae/sim3_new.png}
        \subcaption{生成構造}
    \end{minipage}
\caption{検証構造と同様の骨格を持った構造を生成できたパターン} \label{fig:aae_similar}
\end{figure}

\secref{sec:result_vae}に比べて良い結果が出た理由として、AAE自体の特徴と段階の少なさの2点が挙げられる。

VAEは正則化のロスを解析的に計算する必要があるため、各サンプルに対応するのは分布の平均と分散である。
すなわち、潜在変数で同じ点であっても、複数のサンプルの分布が重なりあう可能性がある。
1つの点が複数の構造に対応する場合、復元誤差が大きくなり、学習が不安定になる。
一方、AAEは正則化にDiscriminatorを利用するため、各サンプルは1点と対応させれば十分である。
1サンプルと1点が対応するため、Decoderの学習がスムーズに進むと考えられる。

段階の数という点に着目すると、提案手法で用いたVAEはM1、M22段階のモデルであり、\secref{sec:result_vae}で述べたように復元誤差が蓄積する問題があると考えられる。
AAEでは1段階で教師付きデータと教師なしデータを学習させることができるため、復元誤差が乗る余地が少ない。
以上の理由により、VAEよりもAAEの方が良い結果を示したと考えられる。

\subsection{課題と解決の方針}

\subsubsection{骨格の多様性の低さ}

AAEによって生成される構造には、訓練データに類似の構造が含まれるという欠点があった(\figref{fig:aae_identical})。
AAEによって妥当な構造生成はできているものの、生成された構造の骨格の多様性については課題が残るということが分かった。

これは活性値情報が存在するデータの少なさに由来すると考えられる。
活性値情報が存在するデータが少ない場合、活性を落とさずに行うことができる構造変化のパターンの数が少なくなる。
構造変化のパターンが少ないため、骨格から変化させた構造を生成することが難しい。
逆に考えれば、構造変化のパターンを増やすためには活性値情報付きのデータを増やす必要がある。
タンパクを絞って活性値情報を増やすのは難しいため、類似するタンパクに対するリガンドデータを用いてマルチモーダル的に学習を行うことで
構造変化のパターンを増やすことができると期待される。

副作用の検討も同様の枠組みでできると考えられる。
結合してほしくないタンパクに対するリガンドデータを学習データに加え、Discriminatorにラベルを与えることで
潜在変数空間上で望ましい構造と望ましくない構造を分離することができると考えられる。

\subsubsection{GANの枠組み全体に共通する学習の難しさ}

また、GANの枠組みは一般に学習が難しいことが知られている。
本研究の場合も同様で、\secref{sec:aae_architecture}に示した通り、最適化手法ひとつとっても
Encoder、Decoder、Discriminatorすべての学習がうまく進むようにパラメータを調節する必要がある。
各ネットワークのパラメータについても同様で、パラメータを変更すると学習がうまく進まなかったり、
場合によってはパラメータを変更していなくても学習が失敗することさえある。
一般にランダム性を含んだモデルの場合、乱数のシード値を固定することで再現性を担保することができる。
しかし、深層学習モデルを複数のGPUを用いて学習を行った場合、乱数のシード値を固定していても厳密な再現を取ることが難しい。

GANの学習を安定させるテクニックは近年多く提案されている\cite{gan_techniques, Neyshabur2017}が、
本研究では試せていない。これらのテクニックを適用することで改善が見込める可能性がある。
